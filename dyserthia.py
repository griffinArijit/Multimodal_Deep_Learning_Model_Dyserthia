# -*- coding: utf-8 -*-
"""Dyserthia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J6e6D4XJrXW_f4Hq5O6wBt5IQon2Y0dc
"""



# prompt: open Files in the dataset directory: ['data_with_path.csv', 'Dysarthria and Non Dysarthria']

# The relevant files identified are 'data_with_path.csv' and 'Dysarthria and Non Dysarthria'.
# We can open the CSV file using pandas.
import pandas as pd

# Define the name of the CSV file
csv_file_to_open = 'data_with_path.csv'

# Construct the full path to the CSV file
full_csv_file_path = os.path.join(path, csv_file_to_open)

# Check if the CSV file exists before trying to open it
if os.path.exists(full_csv_file_path):
    try:
        # Open and read the CSV file into a pandas DataFrame
        df = pd.read_csv(full_csv_file_path)
        print(f"Content of {csv_file_to_open} (first 5 rows):\n")
        print(df.head())
        print("\nDataFrame Info:")
        df.info()
    except Exception as e:
        print(f"Error opening or reading CSV file {csv_file_to_open}: {e}")
else:
    print(f"CSV file not found: {full_csv_file_path}")

# The directory 'Dysarthria and Non Dysarthria' likely contains audio files.
# We can list the files within this directory.
audio_dir_name = 'Dysarthria and Non Dysarthria'
full_audio_dir_path = os.path.join(path, audio_dir_name)

# Check if the audio directory exists
if os.path.isdir(full_audio_dir_path):
    print(f"\nFiles in directory: {audio_dir_name}")
    try:
        # List files in the audio directory
        audio_files = os.listdir(full_audio_dir_path)
        print(audio_files[:10]) # Print the first 10 files
        print(f"...and {len(audio_files)} more files.")
    except Exception as e:
        print(f"Error listing files in directory {audio_dir_name}: {e}")
else:
    print(f"Directory not found: {full_audio_dir_path}")

# To open and process the actual audio files within 'Dysarthria and Non Dysarthria',
# you would typically need a library like `librosa` or `scipy.io.wavfile`.
# For example:
# !pip install librosa
# import librosa
# # Assuming 'some_audio_file.wav' is one of the files in the directory
# example_audio_file = os.path.join(full_audio_dir_path, 'some_audio_file.wav')
# if os.path.exists(example_audio_file):
#     y, sr = librosa.load(example_audio_file)
#     print(f"Loaded audio file: {example_audio_file}")
#     print(f"Shape of audio data: {y.shape}")
#     print(f"Sampling rate: {sr}")
# else:
#      print(f"Example audio file not found: {example_audio_file}")

from IPython.display import Audio, display
import os
import pandas as pd

# Assuming df is already defined and contains a 'Wav_path' column

# Check if 'Wav_path' column exists and DataFrame is not empty
if 'df' in globals() and 'Wav_path' in df.columns and not df.empty:
    # Use the paths directly from the DataFrame
    df['full_wav_path'] = df['Wav_path']

    # Check if the full file path exists
    df['file_exists'] = df['full_wav_path'].apply(lambda x: os.path.exists(x) if x else False)

    # Preview path corrections
    print(df[['Wav_path', 'full_wav_path', 'file_exists']].head())

    # Attempt to play the first available audio file
    if df['file_exists'].any():
        first_valid_audio = df[df['file_exists']].iloc[0]['full_wav_path']
        print(f"\nPlaying first available audio file: {first_valid_audio}")
        try:
            display(Audio(first_valid_audio))
        except Exception as e:
            print(f"Error playing audio for {first_valid_audio}: {e}")
    else:
        print("No valid audio files found at the specified paths.")

else:
    print("DataFrame 'df' is not defined, empty, or does not contain a 'Wav_path' column.")

import os
import kagglehub

# Re-run the download command to get the latest path information
path = kagglehub.dataset_download("poojag718/dysarthria-and-nondysarthria-speech-dataset")
print("Path to dataset files from kagglehub:", path)

# List files in the downloaded directory to understand the structure
files_in_download_path = os.listdir(path)
print("Files in the downloaded dataset directory:", files_in_download_path)

# Check inside the 'Dysarthria and Non Dysarthria' directory if it exists
audio_dir_name = 'Dysarthria and Non Dysarthria'
full_audio_dir_path = os.path.join(path, audio_dir_name)

if os.path.isdir(full_audio_dir_path):
    print(f"\nFiles in directory: {audio_dir_name}")
    try:
        # List files in the audio directory
        audio_files_in_subdir = os.listdir(full_audio_dir_path)
        print(audio_files_in_subdir[:10]) # Print the first 10 files
        print(f"...and {len(audio_files_in_subdir)} more files.")
    except Exception as e:
        print(f"Error listing files in directory {audio_dir_name}: {e}")
else:
    print(f"Directory not found: {full_audio_dir_path}")

import whisper
import os
import pandas as pd
from tqdm import tqdm

# Load Whisper model
asr_model = whisper.load_model("base")

# Add Transcription column if not present
if 'Transcription' not in df.columns:
    df['Transcription'] = None

# Prepare log file
log_file = "transcription_log.txt"
with open(log_file, "w", encoding="utf-8") as log:
    log.write("Transcription Log\n")
    log.write("=================\n\n")

    # Process each file
    for i in tqdm(df.index):
        try:
            path = df.loc[i, 'Wav_path']
            prompt = df.loc[i, 'Prompts']  # Get the prompt

            if os.path.exists(path):
                # Transcribe
                result = asr_model.transcribe(path)
                transcript = result.get('text', '').strip()
                df.loc[i, 'Transcription'] = transcript if transcript else "[Empty transcript]"

                message = (
                    f"âœ… Row {i}\n"
                    f"ğŸ—£ï¸ Prompt: {prompt}\n"
                    f"ğŸ“ Transcription: {transcript}\n"
                )
            else:
                df.loc[i, 'Transcription'] = "[Missing file]"
                message = (
                    f"âš ï¸ Row {i}\n"
                    f"ğŸ—£ï¸ Prompt: {prompt}\n"
                    f"ğŸš« File not found: {path}\n"
                )

        except Exception as e:
            df.loc[i, 'Transcription'] = f"[Error: {str(e)}]"
            prompt = df.loc[i, 'Prompts']
            message = (
                f"âŒ Row {i}\n"
                f"ğŸ—£ï¸ Prompt: {prompt}\n"
                f"ğŸš« Error: {str(e)}\n"
            )

        # Print and log
        print(message)
        log.write(message + "\n")

df

# Save full df as CSV in a folder like MyDrive/Transcriptions
df.to_csv("/content/drive/MyDrive/full_transcription_dataframe.csv", index=False)



"""TORGO

"""

# prompt: df /content/drive/MyDrive/finale_data_with_transcription.pkl

import pandas as pd
df = pd.read_pickle('/content/drive/MyDrive/finale_data_with_transcription.pkl')

# ğŸ“¦ Imports
import os
import numpy as np
import pandas as pd
import librosa
import tensorflow as tf
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models, Input

# ğŸ§  STFT Feature Extraction Function
def extract_stft(path, shape=(128, 128)):
    try:
        y, sr = librosa.load(path, sr=22050)
        D = librosa.stft(y, n_fft=2048, hop_length=512)
        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
        S_db = S_db[:shape[0], :shape[1]] if S_db.shape[1] >= shape[1] else np.pad(
            S_db, ((0, max(0, shape[0] - S_db.shape[0])), (0, shape[1] - S_db.shape[1])), mode='constant')
        S_db = (S_db - S_db.min()) / (S_db.max() - S_db.min())  # normalize
        return np.stack([S_db] * 3, axis=-1)  # (128, 128, 3)
    except Exception as e:
        print(f"âš ï¸ Error processing {path}: {e}")
        return None
path = kagglehub.dataset_download("poojag718/dysarthria-and-nondysarthria-speech-dataset")
# ğŸ§± Data Preparation
X_images, X_meta, y_labels = [], [], []

for _, row in tqdm(df.iterrows(), total=len(df)):
    local_path = row['Wav_path'].replace("/kaggle/input/dysarthria-and-nondysarthria-speech-dataset", path)
    if not os.path.exists(local_path):
        continue
    spec = extract_stft(local_path)
    if spec is not None and spec.shape == (128, 128, 3):
        X_images.append(spec)
        X_meta.append([
            row["Whisper_Similarity"] / 100.0,
            float(row["Was_Retry_Used"])
        ])
        y_labels.append(row["Is_dysarthria"])

X_images = np.array(X_images)
X_meta = np.array(X_meta)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_labels)

# ğŸ”€ Train-Test Split
X_img_train, X_img_test, X_meta_train, X_meta_test, y_train, y_test = train_test_split(
    X_images, X_meta, y, test_size=0.2, random_state=42, stratify=y
)

# ğŸ—ï¸ Custom CNN + Metadata Model
image_input = Input(shape=(128, 128, 3), name="stft_image")
x = layers.Conv2D(32, 3, activation='relu', padding='same')(image_input)
x = layers.MaxPooling2D(2)(x)
x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D(2)(x)
x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D(2)(x)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.4)(x)
image_branch = x

meta_input = Input(shape=(2,), name="meta_features")
m = layers.Dense(32, activation='relu')(meta_input)
m = layers.Dense(16, activation='relu')(m)
m = layers.Dropout(0.3)(m)
meta_branch = m

combined = layers.Concatenate()([image_branch, meta_branch])
z = layers.Dense(64, activation='relu')(combined)
z = layers.Dropout(0.4)(z)
z = layers.Dense(32, activation='relu')(z)
output = layers.Dense(1, activation='sigmoid')(z)

model = models.Model(inputs=[image_input, meta_input], outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# ğŸ¯ Training
history = model.fit(
    {"stft_image": X_img_train, "meta_features": X_meta_train},
    y_train,
    validation_data=(
        {"stft_image": X_img_test, "meta_features": X_meta_test}, y_test
    ),
    epochs=30,
    batch_size=8
)

# ğŸ“Š Evaluation
loss, acc = model.evaluate(
    {"stft_image": X_img_test, "meta_features": X_meta_test}, y_test
)
print(f"âœ… Final Test Accuracy: {acc * 100:.2f}%")

# ğŸ“¦ Imports
import os
import numpy as np
import pandas as pd
import librosa
import tensorflow as tf
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models, Input

# ğŸ§  STFT Feature Extraction
def extract_stft(path, shape=(128, 128)):
    try:
        y, sr = librosa.load(path, sr=22050)
        D = librosa.stft(y, n_fft=2048, hop_length=512)
        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
        S_db = S_db[:shape[0], :shape[1]] if S_db.shape[1] >= shape[1] else np.pad(
            S_db, ((0, max(0, shape[0] - S_db.shape[0])), (0, shape[1] - S_db.shape[1])), mode='constant')
        S_db = (S_db - S_db.min()) / (S_db.max() - S_db.min())  # normalize to [0, 1]
        return np.stack([S_db] * 3, axis=-1)
    except Exception as e:
        print(f"âš ï¸ Error processing {path}: {e}")
        return None

# ğŸ“‚ Dataset path
path = kagglehub.dataset_download("poojag718/dysarthria-and-nondysarthria-speech-dataset")

# ğŸ“Š Data Preparation
X_images, X_meta, y_labels = [], [], []

for _, row in tqdm(df.iterrows(), total=len(df)):
    local_path = row['Wav_path'].replace("/kaggle/input/dysarthria-and-nondysarthria-speech-dataset", path)
    if not os.path.exists(local_path):
        continue
    spec = extract_stft(local_path)
    if spec is not None and spec.shape == (128, 128, 3):
        X_images.append(spec)
        X_meta.append([
            row["Whisper_Similarity"] / 100.0,
            float(row["Was_Retry_Used"])
        ])
        y_labels.append(row["Is_dysarthria"])

X_images = np.array(X_images)
X_meta = np.array(X_meta)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_labels)

# ğŸ”€ 60% Train / 20% Val / 20% Test Split
X_img_temp, X_img_test, X_meta_temp, X_meta_test, y_temp, y_test = train_test_split(
    X_images, X_meta, y, test_size=0.2, random_state=42, stratify=y
)
X_img_train, X_img_val, X_meta_train, X_meta_val, y_train, y_val = train_test_split(
    X_img_temp, X_meta_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)

# ğŸ§± Hybrid CNN + Metadata Model
image_input = Input(shape=(128, 128, 3), name="stft_image")
x = layers.Conv2D(32, 3, activation='relu', padding='same')(image_input)
x = layers.MaxPooling2D(2)(x)
x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D(2)(x)
x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D(2)(x)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.4)(x)
image_branch = x

meta_input = Input(shape=(2,), name="meta_features")
m = layers.Dense(32, activation='relu')(meta_input)
m = layers.Dense(16, activation='relu')(m)
m = layers.Dropout(0.3)(m)
meta_branch = m

combined = layers.Concatenate()([image_branch, meta_branch])
z = layers.Dense(64, activation='relu')(combined)
z = layers.Dropout(0.4)(z)
z = layers.Dense(32, activation='relu')(z)
output = layers.Dense(1, activation='sigmoid')(z)

model = models.Model(inputs=[image_input, meta_input], outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# ğŸ“‹ Summary
model.summary()

# ğŸ¯ Train
history = model.fit(
    {"stft_image": X_img_train, "meta_features": X_meta_train},
    y_train,
    validation_data=(
        {"stft_image": X_img_val, "meta_features": X_meta_val}, y_val
    ),
    epochs=30,
    batch_size=8
)

# ğŸ“Š Evaluate on Test Set
loss, acc = model.evaluate(
    {"stft_image": X_img_test, "meta_features": X_meta_test}, y_test
)
print(f"âœ… Final Test Accuracy: {acc * 100:.2f}%")

# ğŸ“¦ Imports
import os
import numpy as np
import pandas as pd
import librosa
import tensorflow as tf
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models, Input
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score
import matplotlib.pyplot as plt
import kagglehub

# ğŸ§  STFT Feature Extraction
def extract_stft(path, shape=(128, 128)):
    try:
        y, sr = librosa.load(path, sr=22050)
        D = librosa.stft(y, n_fft=2048, hop_length=512)
        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
        S_db = S_db[:shape[0], :shape[1]] if S_db.shape[1] >= shape[1] else np.pad(
            S_db, ((0, max(0, shape[0] - S_db.shape[0])), (0, shape[1] - S_db.shape[1])), mode='constant'
        )
        S_db = (S_db - S_db.min()) / (S_db.max() - S_db.min())  # normalize 0..1
        return np.stack([S_db] * 3, axis=-1)  # (128,128,3) for CNN
    except Exception as e:
        print(f"âš ï¸ Error processing {path}: {e}")
        return None

# ğŸ“‚ Dataset path (no CSV read; assumes you already have `df`)
path = kagglehub.dataset_download("poojag718/dysarthria-and-nondysarthria-speech-dataset")

# âœ… Build arrays from your existing `df`
X_images, X_meta, y_labels = [], [], []
for _, row in tqdm(df.iterrows(), total=len(df)):
    local_path = row['Wav_path'].replace("/kaggle/input/dysarthria-and-nondysarthria-speech-dataset", path)
    if not os.path.exists(local_path):
        continue
    spec = extract_stft(local_path)
    if spec is not None and spec.shape == (128, 128, 3):
        X_images.append(spec)
        X_meta.append([row["Whisper_Similarity"] / 100.0, float(row["Was_Retry_Used"])])
        y_labels.append(row["Is_dysarthria"])

X_images = np.array(X_images, dtype=np.float32)
X_meta = np.array(X_meta, dtype=np.float32)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_labels).astype(np.int32)

# ğŸ”€ K-Fold Setup
k = 5
kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
fold = 1
acc_per_fold = []

best_fold = 0
best_acc = 0.0
best_history = None
best_y_true = None
best_y_pred = None
best_X_img_val = None
best_X_meta_val = None
best_model = None

def build_model():
    # ğŸ—ï¸ Custom CNN + Metadata Model (name last conv!)
    image_input = Input(shape=(128, 128, 3), name="stft_image")
    x = layers.Conv2D(32, 3, activation='relu', padding='same')(image_input)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Conv2D(128, 3, activation='relu', padding='same', name="last_conv")(x)  # ğŸ”‘
    x = layers.MaxPooling2D(2)(x)
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.4)(x)
    image_branch = x

    meta_input = Input(shape=(2,), name="meta_features")
    m = layers.Dense(32, activation='relu')(meta_input)
    m = layers.Dense(16, activation='relu')(m)
    m = layers.Dropout(0.3)(m)
    meta_branch = m

    combined = layers.Concatenate()([image_branch, meta_branch])
    z = layers.Dense(64, activation='relu')(combined)
    z = layers.Dropout(0.4)(z)
    z = layers.Dense(32, activation='relu')(z)
    output = layers.Dense(1, activation='sigmoid')(z)

    model = models.Model(inputs=[image_input, meta_input], outputs=output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# ğŸ” K-Fold Training Loop
for train_idx, val_idx in kf.split(X_images, y):
    print(f"\nğŸ” Fold {fold} / {k}")

    X_img_train, X_img_val = X_images[train_idx], X_images[val_idx]
    X_meta_train, X_meta_val = X_meta[train_idx], X_meta[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    model = build_model()
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    history = model.fit(
        {"stft_image": X_img_train, "meta_features": X_meta_train},
        y_train,
        validation_data=({"stft_image": X_img_val, "meta_features": X_meta_val}, y_val),
        epochs=30,
        batch_size=8,
        callbacks=[early_stop],
        verbose=1
    )

    # ğŸ“Š Evaluate
    loss, acc = model.evaluate({"stft_image": X_img_val, "meta_features": X_meta_val}, y_val, verbose=0)
    print(f"âœ… Fold {fold} Accuracy: {acc * 100:.2f}%")
    acc_per_fold.append(acc)

    y_val_pred = model.predict({"stft_image": X_img_val, "meta_features": X_meta_val}, verbose=0).reshape(-1)
    y_val_pred_label = (y_val_pred >= 0.5).astype(int)

    # ğŸ† Track best fold + clone model so later folds don't overwrite
    if acc > best_acc:
        best_acc = acc
        best_fold = fold
        best_history = history
        best_y_true = y_val.copy()
        best_y_pred = y_val_pred_label.copy()
        best_X_img_val = X_img_val.copy()
        best_X_meta_val = X_meta_val.copy()
        best_model = tf.keras.models.clone_model(model)
        best_model.set_weights(model.get_weights())

    fold += 1

# ===========================
# ğŸ“ˆ Curves & Confusion Matrix for Best Fold
# ===========================
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(best_history.history['accuracy'], label='Train Accuracy')
plt.plot(best_history.history['val_accuracy'], label='Val Accuracy')
plt.title(f'Fold {best_fold} Accuracy')
plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend()

plt.subplot(1, 2, 2)
plt.plot(best_history.history['loss'], label='Train Loss')
plt.plot(best_history.history['val_loss'], label='Val Loss')
plt.title(f'Fold {best_fold} Loss')
plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend()
plt.tight_layout(); plt.show()

cm = confusion_matrix(best_y_true, best_y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Non-Dysarthria", "Dysarthria"])
disp.plot(cmap=plt.cm.Blues)
plt.title(f'Fold {best_fold} Confusion Matrix')
plt.show()

f1 = f1_score(best_y_true, best_y_pred)
print(f"F1 Score for Fold {best_fold}: {f1:.4f}")
print("\nğŸ“Š Average Accuracy across folds: {:.2f}%".format(np.mean(acc_per_fold) * 100))
print(f"ğŸ† Using best fold: {best_fold} (Accuracy: {best_acc*100:.2f}%)")

# ===========================
# ğŸ”¥ Grad-CAM (on best_model)
# ===========================
def make_gradcam_heatmap(img_batch, meta_batch, model, last_conv_layer_name="last_conv"):
    """
    img_batch: (B, 128, 128, 3)
    meta_batch: (B, 2)
    For binary sigmoid output, we take the score for class-1 (positive class).
    """
    # Model that maps inputs -> (last conv activations, predictions)
    grad_model = tf.keras.models.Model(
        inputs=[model.get_layer("stft_image").input, model.get_layer("meta_features").input],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output],
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model([img_batch, meta_batch], training=False)
        # For binary sigmoid, predicted score for class 1 is predictions[:, 0]
        class_channel = predictions[:, 0]

    # Gradients of the positive class wrt conv feature maps
    grads = tape.gradient(class_channel, conv_outputs)  # shape: (B, H, W, C)
    # Global-average pooling across H,W -> importance weights per channel
    pooled_grads = tf.reduce_mean(grads, axis=(1, 2))  # shape: (B, C)

    conv_outputs = conv_outputs.numpy()
    pooled_grads = pooled_grads.numpy()

    heatmaps = []
    for i in range(conv_outputs.shape[0]):
        conv_out = conv_outputs[i]           # (H, W, C)
        weights = pooled_grads[i]            # (C,)
        cam = np.dot(conv_out, weights)      # (H, W)
        cam = np.maximum(cam, 0)
        cam /= (cam.max() + 1e-8)
        heatmaps.append(cam)
    return np.array(heatmaps)  # (B, H, W)

def show_gradcam_overlay(img, heatmap, alpha=0.4):
    """
    img: (128,128,3) in [0,1]
    heatmap: (h,w) -> resized to (128,128)
    """
    h = np.uint8(255 * heatmap)[..., None]
    h = tf.image.resize(h, (img.shape[0], img.shape[1])).numpy().astype(np.uint8).squeeze()

    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1); plt.title("Original Spectrogram"); plt.imshow(img); plt.axis("off")
    plt.subplot(1, 2, 2); plt.title("Grad-CAM (class=1)"); plt.imshow(img); plt.imshow(h, cmap="jet", alpha=alpha); plt.axis("off")
    plt.tight_layout(); plt.show()

# ğŸ¯ Choose samples from best fold (misclassified first)
mis_idx = np.where(best_y_true != best_y_pred)[0]
sel_idx = mis_idx[:3] if len(mis_idx) > 0 else np.arange(min(3, len(best_y_true)))

if len(sel_idx) == 0:
    print("No validation samples available for Grad-CAM.")
else:
    img_batch = best_X_img_val[sel_idx]
    meta_batch = best_X_meta_val[sel_idx]
    heatmaps = make_gradcam_heatmap(img_batch, meta_batch, best_model, last_conv_layer_name="last_conv")

    for i, idx in enumerate(sel_idx):
        print(f"Sample idx {idx} | True: {best_y_true[idx]} | Pred: {best_y_pred[idx]}")
        show_gradcam_overlay(best_X_img_val[idx], heatmaps[i], alpha=0.45)

def make_gradcam_heatmap(img_batch, meta_batch, model, last_conv_layer_name="conv2d_26"):
    """
    img_batch: (B, 128, 128, 3)
    meta_batch: (B, 2)
    For binary sigmoid output, we take the score for class-1 (positive class).
    """
    # Build a model that maps inputs -> (last conv outputs, predictions)
    grad_model = tf.keras.models.Model(
        inputs=model.inputs,
        outputs=[model.get_layer(last_conv_layer_name).output, model.output],
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model([img_batch, meta_batch], training=False)
        # For binary sigmoid, use predictions[:, 0] as "positive class" score
        class_channel = predictions[:, 0]

    # Gradient of class wrt conv feature maps
    grads = tape.gradient(class_channel, conv_outputs)  # shape: (B, H, W, C)
    pooled_grads = tf.reduce_mean(grads, axis=(1, 2))   # (B, C)

    conv_outputs = conv_outputs.numpy()
    pooled_grads = pooled_grads.numpy()

    heatmaps = []
    for i in range(conv_outputs.shape[0]):
        conv_out = conv_outputs[i]         # (H, W, C)
        weights = pooled_grads[i]          # (C,)
        cam = np.dot(conv_out, weights)    # (H, W)
        cam = np.maximum(cam, 0)
        cam /= (cam.max() + 1e-8)
        heatmaps.append(cam)

    return np.array(heatmaps)  # (B, H, W)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# ===========================
# ğŸ¯ Select random samples by outcome type (TP, FP, TN, FN)
# ===========================
y_true = np.array(best_y_true)
y_pred = np.array(best_y_pred)

tp_idx = np.where((y_true == 1) & (y_pred == 1))[0]
fp_idx = np.where((y_true == 0) & (y_pred == 1))[0]
tn_idx = np.where((y_true == 0) & (y_pred == 0))[0]
fn_idx = np.where((y_true == 1) & (y_pred == 0))[0]

# Randomly pick up to N samples from each class
N = 2  # change to show more/less
rng = np.random.default_rng(seed=42)  # reproducible

sel_dict = {
    "True Positive": rng.choice(tp_idx, size=min(N, len(tp_idx)), replace=False) if len(tp_idx) > 0 else [],
    "False Positive": rng.choice(fp_idx, size=min(N, len(fp_idx)), replace=False) if len(fp_idx) > 0 else [],
    "True Negative": rng.choice(tn_idx, size=min(N, len(tn_idx)), replace=False) if len(tn_idx) > 0 else [],
    "False Negative": rng.choice(fn_idx, size=min(N, len(fn_idx)), replace=False) if len(fn_idx) > 0 else [],
}

# ===========================
# ğŸ”¥ Grad-CAM Visualization per Category
# ===========================
for label, idx_list in sel_dict.items():
    if len(idx_list) == 0:
        print(f"No samples for {label}")
        continue

    img_batch = best_X_img_val[idx_list]
    meta_batch = best_X_meta_val[idx_list]

    heatmaps = make_gradcam_heatmap(img_batch, meta_batch, best_model, last_conv_layer_name="last_conv")

    for i, idx in enumerate(idx_list):
        print(f"[{label}] Sample idx {idx} | True: {best_y_true[idx]} | Pred: {best_y_pred[idx]}")
        show_gradcam_overlay(best_X_img_val[idx], heatmaps[i], alpha=0.45)

# ===========================
# ğŸ“Š Confusion Matrix
# ===========================
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Non-Dysarthria", "Dysarthria"])

fig, ax = plt.subplots(figsize=(6, 5))
disp.plot(ax=ax, cmap="Blues", colorbar=True, values_format="d")
plt.title("Confusion Matrix")
plt.show()

# Suppose you already have these from validation:
# best_X_img_val, best_X_meta_val, best_y_true, best_y_pred, best_model

# Select 20 True Positives
tp_idx = np.where((best_y_true == 1) & (best_y_pred == 1))[0]
sel_tp_idx = tp_idx[:20]

plot_gradcam_grid(best_X_img_val, best_X_meta_val, best_model, sel_tp_idx,
                  title="True Positive Grad-CAMs", last_conv_layer_name="conv2d_19")

model.summary()

import os
import numpy as np
import pandas as pd
import librosa
import tensorflow as tf
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score
import matplotlib.pyplot as plt
from tensorflow.keras import layers, models, Input
from tensorflow.keras.callbacks import EarlyStopping
import kagglehub

# ğŸ§  STFT Feature Extraction
def extract_stft(path, shape=(128, 128)):
    try:
        y, sr = librosa.load(path, sr=22050)
        D = librosa.stft(y, n_fft=2048, hop_length=512)
        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
        S_db = S_db[:shape[0], :shape[1]] if S_db.shape[1] >= shape[1] else np.pad(
            S_db, ((0, max(0, shape[0] - S_db.shape[0])), (0, shape[1] - S_db.shape[1])), mode='constant')
        S_db = (S_db - S_db.min()) / (S_db.max() - S_db.min())  # normalize
        return np.stack([S_db] * 3, axis=-1)
    except Exception as e:
        print(f"âš ï¸ Error processing {path}: {e}")
        return None

# ğŸ“‚ Dataset path
path = kagglehub.dataset_download("poojag718/dysarthria-and-nondysarthria-speech-dataset")


# ğŸ“Š Data Preparation
X_images, X_meta, y_labels = [], [], []
for _, row in tqdm(df.iterrows(), total=len(df)):
    local_path = row['Wav_path'].replace("/kaggle/input/dysarthria-and-nondysarthria-speech-dataset", path)
    if not os.path.exists(local_path):
        continue
    spec = extract_stft(local_path)
    if spec is not None and spec.shape == (128, 128, 3):
        X_images.append(spec)
        X_meta.append([
            row["Whisper_Similarity"] / 100.0,
            float(row["Was_Retry_Used"])
        ])
        y_labels.append(row["Is_dysarthria"])

X_images = np.array(X_images)
X_meta = np.array(X_meta)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_labels)

# ğŸ”€ K-Fold Setup
k = 5
kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
fold = 1
acc_per_fold = []

best_fold = 0
best_acc = 0
best_history = None
best_y_true = []
best_y_pred = []

# â±ï¸ Optional: Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# ğŸ” Evaluate CNN-only and Metadata-only models
print("\nğŸ“Œ Evaluating CNN-only and Metadata-only Models Separately...")

cnn_acc, cnn_f1 = [], []
meta_acc, meta_f1 = [], []

for train_idx, val_idx in kf.split(X_images, y):
    X_img_train, X_img_val = X_images[train_idx], X_images[val_idx]
    X_meta_train, X_meta_val = X_meta[train_idx], X_meta[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    # -------- CNN Only --------
    cnn_input = Input(shape=(128, 128, 3))
    x = layers.Conv2D(32, 3, activation='relu', padding='same')(cnn_input)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.4)(x)
    out = layers.Dense(1, activation='sigmoid')(x)

    cnn_model = models.Model(inputs=cnn_input, outputs=out)
    cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    cnn_model.fit(X_img_train, y_train, validation_data=(X_img_val, y_val),
                  epochs=30, batch_size=8, callbacks=[early_stop], verbose=0)
    y_pred_cnn = cnn_model.predict(X_img_val) >= 0.5
    cnn_acc.append(np.mean(y_pred_cnn.flatten() == y_val))
    cnn_f1.append(f1_score(y_val, y_pred_cnn.flatten()))

    # -------- Metadata Only --------
    meta_input = Input(shape=(2,))
    m = layers.Dense(32, activation='relu')(meta_input)
    m = layers.Dense(16, activation='relu')(m)
    m = layers.Dropout(0.3)(m)
    out = layers.Dense(1, activation='sigmoid')(m)

    meta_model = models.Model(inputs=meta_input, outputs=out)
    meta_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    meta_model.fit(X_meta_train, y_train, validation_data=(X_meta_val, y_val),
                   epochs=30, batch_size=8, callbacks=[early_stop], verbose=0)
    y_pred_meta = meta_model.predict(X_meta_val) >= 0.5
    meta_acc.append(np.mean(y_pred_meta.flatten() == y_val))
    meta_f1.append(f1_score(y_val, y_pred_meta.flatten()))

# ğŸ“Š Report Results
print("\nğŸ§  CNN-only Model:")
print(f"â†’ Accuracy: {np.mean(cnn_acc) * 100:.2f}%")
print(f"â†’ F1 Score: {np.mean(cnn_f1):.4f}")

print("\nğŸ“‹ Metadata-only Model:")
print(f"â†’ Accuracy: {np.mean(meta_acc) * 100:.2f}%")
print(f"â†’ F1 Score: {np.mean(meta_f1):.4f}")

# ğŸ“¦ Imports
import os
import numpy as np
import pandas as pd
import librosa
from tqdm import tqdm
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models, Input

# ğŸšï¸ Parameters
AUDIO_LEN = 8192
SR = 22050

# ğŸ“¥ Load Raw Audio

def load_audio_raw(path, length=AUDIO_LEN):
    try:
        y, _ = librosa.load(path, sr=SR)
        if len(y) < length:
            y = np.pad(y, (0, length - len(y)))
        else:
            y = y[:length]
        return y
    except Exception as e:
        print(f"âš ï¸ Error loading {path}: {e}")
        return None

# ğŸ“‚ Dataset path
path = kagglehub.dataset_download("poojag718/dysarthria-and-nondysarthria-speech-dataset")

# ğŸ§± Load Data
X_audio, X_meta, y_labels = [], [], []

for _, row in tqdm(df.iterrows(), total=len(df)):
    local_path = row['Wav_path'].replace("/kaggle/input/dysarthria-and-nondysarthria-speech-dataset", path)
    if not os.path.exists(local_path):
        continue
    y = load_audio_raw(local_path)
    if y is not None:
        X_audio.append(y)
        X_meta.append([
            row["Whisper_Similarity"] / 100.0,
            float(row["Was_Retry_Used"])
        ])
        y_labels.append(row["Is_dysarthria"])

# ğŸ”¢ Convert Arrays
X_audio = np.expand_dims(np.array(X_audio), -1)  # (N, 8192, 1)
X_meta = np.array(X_meta)
y = LabelEncoder().fit_transform(y_labels)

# ğŸ”€ Split Data (60/20/20)
X_a_train, X_a_temp, X_m_train, X_m_temp, y_train, y_temp = train_test_split(
    X_audio, X_meta, y, test_size=0.4, stratify=y, random_state=42)
X_a_val, X_a_test, X_m_val, X_m_test, y_val, y_test = train_test_split(
    X_a_temp, X_m_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

def build_model():
    # Inputs
    raw_input = Input(shape=(AUDIO_LEN, 1), name='raw_audio')
    meta_input = Input(shape=(2,), name='meta_features')

    # Pre-emphasis
    pre_emph = layers.Conv1D(1, kernel_size=5, padding='same', use_bias=False, trainable=True)(raw_input)

    # STFT branch
    def stft_fn(x):
        stft = tf.signal.stft(tf.squeeze(x, -1), frame_length=1280, frame_step=380, fft_length=1280,
                              window_fn=tf.signal.hamming_window)
        mag = tf.math.log(tf.abs(stft) + 1e-6)
        return tf.expand_dims(mag, -1)

    stft_feat = layers.Lambda(stft_fn, name="stft_branch")(pre_emph)

    x1 = layers.Conv2D(12, 5, padding='same')(stft_feat)
    x1 = layers.BatchNormalization()(x1)
    x1 = layers.ReLU()(x1)
    x1 = layers.MaxPooling2D(pool_size=3, strides=2)(x1)

    x1 = layers.Conv2D(24, 3, padding='same')(x1)
    x1 = layers.BatchNormalization()(x1)
    x1 = layers.ReLU()(x1)
    x1 = layers.MaxPooling2D(pool_size=3, strides=2)(x1)

    x1 = layers.Conv2D(48, 3, padding='same')(x1)
    x1 = layers.BatchNormalization()(x1)
    x1 = layers.ReLU()(x1)
    x1 = layers.MaxPooling2D(pool_size=3, strides=2)(x1)
    x1 = layers.Flatten()(x1)

    # Mel branch
    def mel_fn(x):
        y = tf.squeeze(x, -1)
        stft = tf.signal.stft(y, frame_length=1280, frame_step=380, fft_length=1280,
                              window_fn=tf.signal.hamming_window)
        mag = tf.abs(stft)
        mel_matrix = tf.signal.linear_to_mel_weight_matrix(
            num_mel_bins=128, num_spectrogram_bins=641, sample_rate=SR,
            lower_edge_hertz=80.0, upper_edge_hertz=7600.0
        )
        mel_spec = tf.tensordot(mag, mel_matrix, axes=1)
        log_mel = tf.math.log(mel_spec + 1e-6)
        return tf.expand_dims(log_mel, -1)

    mel_feat = layers.Lambda(mel_fn, name="mel_branch")(pre_emph)

    x2 = layers.Conv2D(12, 5, padding='same')(mel_feat)
    x2 = layers.BatchNormalization()(x2)
    x2 = layers.ReLU()(x2)
    x2 = layers.MaxPooling2D(pool_size=3, strides=2)(x2)

    x2 = layers.Conv2D(24, 3, padding='same')(x2)
    x2 = layers.BatchNormalization()(x2)
    x2 = layers.ReLU()(x2)
    x2 = layers.MaxPooling2D(pool_size=3, strides=2)(x2)

    x2 = layers.Conv2D(48, 3, padding='same')(x2)
    x2 = layers.BatchNormalization()(x2)
    x2 = layers.ReLU()(x2)
    x2 = layers.MaxPooling2D(pool_size=3, strides=2)(x2)
    x2 = layers.Flatten()(x2)

    # Merge STFT + Mel features
    audio_feat = layers.Concatenate()([x1, x2])
    audio_feat = layers.Dense(128, activation='relu')(audio_feat)

    # Metadata branch
    m = layers.Dense(32, activation='relu')(meta_input)
    m = layers.Dense(16, activation='relu')(m)
    m = layers.Dropout(0.3)(m)

    # Merge all
    combined = layers.Concatenate()([audio_feat, m])
    z = layers.Dense(64, activation='relu')(combined)
    z = layers.Dropout(0.4)(z)
    z = layers.Dense(32, activation='relu')(z)
    output = layers.Dense(1, activation='sigmoid')(z)

    # Build model
    model = models.Model(inputs=[raw_input, meta_input], outputs=output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model


# ğŸš€ Train Model
# model.fit(
#     {"raw_audio": X_a_train, "meta_features": X_m_train},
#     y_train,
#     validation_data=({"raw_audio": X_a_val, "meta_features": X_m_val}, y_val),
#     epochs=30,
#     batch_size=8,
#     verbose=1
# )

# # ğŸ“Š Evaluate
# loss, acc = model.evaluate(
#     {"raw_audio": X_a_test, "meta_features": X_m_test}, y_test
# )
# print(f"âœ… Final Test Accuracy: {acc * 100:.2f}%")

# prompt: how many file exist == true

# Count how many times os.path.exists(local_path) is True
file_exists_count = 0
for _, row in df.iterrows():
    local_path = row['Wav_path'].replace("/kaggle/input/dysarthria-and-nondysarthria-speech-dataset", path)
    if os.path.exists(local_path):
        file_exists_count += 1

print(f"Number of files that exist: {file_exists_count}")

df.info()

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
best_model = None
best_acc = 0
best_X_a_val, best_X_m_val, best_y_val = None, None, None # Store validation data for the best fold

for fold, (train_idx, val_idx) in enumerate(kf.split(X_audio, y), 1):
    print(f"\nğŸ” Fold {fold}")

    X_a_train, X_a_val = X_audio[train_idx], X_audio[val_idx]
    X_m_train, X_m_val = X_meta[train_idx], X_meta[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    # ğŸ—ï¸ Build new model per fold
    model = build_model()  # ğŸ‘ˆ Call your build_model() here

    # ğŸ§  Train
    history = model.fit(
        {"raw_audio": X_a_train, "meta_features": X_m_train},
        y_train,
        validation_data=({"raw_audio": X_a_val, "meta_features": X_m_val}, y_val),
        epochs=30,
        batch_size=16,
        verbose=0
    )

    # ğŸ“Š Evaluate
    y_pred = model.predict({"raw_audio": X_a_val, "meta_features": X_m_val})
    y_pred_binary = (y_pred.flatten() > 0.5).astype(int)
    acc = accuracy_score(y_val, y_pred_binary)
    print(f"âœ… Fold {fold} Accuracy: {acc * 100:.2f}%")
    fold_accuracies.append(acc)

    # ğŸ’¾ Track Best Model and its validation data
    if acc > best_acc:
        best_acc = acc
        best_model = model  # Optional: save model weights too
        best_X_a_val, best_X_m_val, best_y_val = X_a_val, X_m_val, y_val


# ğŸ“‰ Overall Result
print(f"\nğŸ“ˆ Average Accuracy: {np.mean(fold_accuracies) * 100:.2f}%")
print(f"ğŸ† Best Fold Accuracy: {best_acc * 100:.2f}%")

# prompt: draw accuracy and loss curve and matrix of best score

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Plot training & validation accuracy values
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Predict probabilities on the validation set from the last fold
y_pred_proba = model.predict({"raw_audio": X_a_val, "meta_features": X_m_val})
# Convert probabilities to binary predictions
y_pred = (y_pred_proba.flatten() > 0.5).astype("int32")

# Generate confusion matrix using validation data from the last fold
cm = confusion_matrix(y_val, y_pred)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix (Last Fold Validation Data)')
plt.show()

# Find the best score (highest validation accuracy from the last fold's history)
best_val_accuracy_last_fold = max(history.history['val_accuracy'])
print(f"\nğŸ† Best Validation Accuracy from the last fold: {best_val_accuracy_last_fold * 100:.2f}%")

# Report K-Fold cross-validation results
if 'acc_per_fold' in locals():
    print(f"ğŸ“ˆ Mean K-Fold Validation Accuracy: {np.mean(acc_per_fold) * 100:.2f}%")
    print(f"ğŸ” Max K-Fold Validation Accuracy: {max(acc_per_fold) * 100:.2f}%")

import pandas as pd
import os
import kagglehub

# Download the dataset
path = kagglehub.dataset_download("poojag718/dysarthria-and-nondysarthria-speech-dataset")

# Load the metadata CSV
csv_path = os.path.join(path, "data_with_path.csv")
df = pd.read_csv(csv_path)
print("âœ… Loaded metadata:", df.shape)

dataset_input_prefix = "/kaggle/input/dysarthria-and-nondysarthria-speech-dataset"
df = df[df['Wav_path'].notna()]
df['full_path'] = df['Wav_path'].apply(lambda x: x.replace(dataset_input_prefix, path))

import os
import numpy as np
import librosa
import tensorflow as tf
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


# Step 2: STFT feature generator
def extract_stft(path, shape=(128, 128)):
    try:
        y, sr = librosa.load(path, sr=22050)
        D = librosa.stft(y, n_fft=2048, hop_length=512)
        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
        # Pad/crop to fixed shape
        S_db = S_db[:shape[0], :shape[1]] if S_db.shape[1] >= shape[1] else np.pad(
            S_db, ((0, max(0, shape[0]-S_db.shape[0])), (0, shape[1]-S_db.shape[1])), mode='constant')
        S_db = (S_db - S_db.min()) / (S_db.max() - S_db.min())  # Normalize
        return np.stack([S_db]*3, axis=-1)  # Convert to RGB
    except Exception as e:
        print(f"âš ï¸ Error processing {path}: {e}")
        return None

# Step 3: Build dataset
X = []
y = []

for i, row in tqdm(df.iterrows(), total=len(df)):
    local_path = row['Wav_path'].replace("/kaggle/input/dysarthria-and-nondysarthria-speech-dataset", path)
    if not os.path.exists(local_path):
        continue  # skip missing
    spec = extract_stft(local_path)
    if spec is not None and spec.shape == (128, 128, 3):
        X.append(spec)
        y.append(row['Is_dysarthria'])


X = np.array(X)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)  # Yes/No â†’ 1/0

print("âœ… Data shape:", X.shape, y.shape)

# Step 4: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 5: STFT-CNN model
def build_stft_cnn(input_shape=(128, 128, 3)):
    inputs = tf.keras.Input(shape=input_shape)
    x = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)
    x = tf.keras.layers.MaxPooling2D(2)(x)
    x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D(2)(x)
    x = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D(2)(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(128, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inputs, output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

model = build_stft_cnn()
model.summary()

# Step 6: Train
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=30,
    batch_size=32
)

# Step 7: Evaluate
loss, acc = model.evaluate(X_test, y_test)
print(f"âœ… Final Test Accuracy: {acc * 100:.2f}%")

import os
import pandas as pd
from IPython.display import Audio, display
import kagglehub

# ğŸ”½ Step 1: Download the dataset
path = kagglehub.dataset_download("poojag718/dysarthria-and-nondysarthria-speech-dataset")
print("âœ… Dataset downloaded to:", path)

# ğŸ“„ Step 2: Load the metadata CSV
csv_path = os.path.join(path, "data_with_path.csv")
df = pd.read_csv(csv_path)
print("âœ… CSV loaded:", df.shape)

# ğŸ§¹ Step 3: Fix path for the first audio file
dataset_input_prefix = "/kaggle/input/dysarthria-and-nondysarthria-speech-dataset"
original_path = df.loc[0, "Wav_path"]
corrected_path = original_path.replace(dataset_input_prefix, path)

# ğŸ§ Step 4: Play the first audio file
if os.path.exists(corrected_path):
    print("ğŸ§ Playing audio from:", corrected_path)
    display(Audio(filename=corrected_path))
else:
    print(f"âŒ File not found: {corrected_path}")

import os
import numpy as np
import pandas as pd
import librosa
import tensorflow as tf
from tqdm import tqdm
from scipy.signal import ricker
from scipy import signal
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import kagglehub

# ğŸ“¦ Load dataset
path = kagglehub.dataset_download("poojag718/dysarthria-and-nondysarthria-speech-dataset")
df = pd.read_csv(os.path.join(path, "data_with_path.csv"))
prefix = "/kaggle/input/dysarthria-and-nondysarthria-speech-dataset"
df['full_path'] = df['Wav_path'].apply(lambda x: x.replace(prefix, path))

# ğŸ§ Extract CWT Spectrogram
def extract_cwt(path, shape=(128, 128)):
    try:
        y, sr = librosa.load(path, sr=22050)
        if len(y) < 512:
            return None
        widths = np.arange(1, shape[0] + 1)
        cwt_matrix = signal.cwt(y[:shape[1]*4], ricker, widths)
        cwt_matrix = (cwt_matrix - cwt_matrix.min()) / (cwt_matrix.max() - cwt_matrix.min())
        cwt_matrix = cwt_matrix[:, :shape[1]]
        if cwt_matrix.shape[1] < shape[1]:
            cwt_matrix = np.pad(cwt_matrix, ((0, 0), (0, shape[1] - cwt_matrix.shape[1])), mode='constant')
        return np.stack([cwt_matrix]*3, axis=-1)
    except Exception as e:
        print(f"âš ï¸ CWT error for {path}: {e}")
        return None

# ğŸ§¹ Build dataset
X, y = [], []
for _, row in tqdm(df.iterrows(), total=len(df)):
    p = row['full_path']
    if not os.path.exists(p): continue
    img = extract_cwt(p)
    if img is not None and img.shape == (128, 128, 3):
        X.append(img)
        y.append(row['Is_dysarthria'])

X = np.array(X)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
print("âœ… Data loaded:", X.shape, y.shape)

# âœ‚ï¸ Train/Test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# ğŸ§  CWT-CNN model
def build_cwt_cnn(input_shape=(128, 128, 3)):
    inputs = tf.keras.Input(shape=input_shape)
    x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)
    x = tf.keras.layers.MaxPooling2D(2)(x)
    x = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)
    x = tf.keras.layers.MaxPooling2D(2)(x)
    x = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu')(x)
    x = tf.keras.layers.MaxPooling2D(2)(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(128, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inputs, output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# ğŸš€ Train the model
model = build_cwt_cnn()
model.summary()
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=30,
    batch_size=32,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)
    ]
)

# ğŸ“Š Evaluate
loss, acc = model.evaluate(X_test, y_test)
print(f"\nâœ… Final Test Accuracy (CWT-CNN): {acc * 100:.2f}%")

!pip install scipy==1.10.1

# ğŸ“¦ Imports
import os
import numpy as np
import pandas as pd
import librosa
from tqdm import tqdm
import tensorflow as tf
from tensorflow.keras import layers, models, Input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# ğŸ§  STFT Feature Extraction
def extract_stft(path, shape=(128, 128)):
    try:
        y, sr = librosa.load(path, sr=22050)
        D = librosa.stft(y, n_fft=2048, hop_length=512)
        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
        S_db = S_db[:shape[0], :shape[1]] if S_db.shape[1] >= shape[1] else np.pad(
            S_db, ((0, max(0, shape[0] - S_db.shape[0])), (0, shape[1] - S_db.shape[1])), mode='constant')
        S_db = (S_db - S_db.min()) / (S_db.max() - S_db.min())  # normalize to [0, 1]
        return np.stack([S_db] * 3, axis=-1)
    except Exception as e:
        print(f"âš ï¸ Error processing {path}: {e}")
        return None

# ğŸ§ª Load Dataset and Extract STFT + Metadata + Labels
X_images, X_meta, y_labels = [], [], []

for _, row in tqdm(df.iterrows(), total=len(df)):
    local_path = row['Wav_path'].replace("/kaggle/input/dysarthria-and-nondysarthria-speech-dataset", path)
    if not os.path.exists(local_path):
        continue
    spec = extract_stft(local_path)
    if spec is not None and spec.shape == (128, 128, 3):
        X_images.append(spec)
        X_meta.append([
            row["Whisper_Similarity"] / 100.0,
            float(row["Was_Retry_Used"])
        ])
        y_labels.append(row["Is_dysarthria"])

X_images = np.array(X_images)
X_meta = np.array(X_meta)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_labels)

# ğŸ§ª Train / Val / Test Split
X_img_temp, X_img_test, X_meta_temp, X_meta_test, y_temp, y_test = train_test_split(
    X_images, X_meta, y, test_size=0.2, random_state=42, stratify=y
)
X_img_train, X_img_val, X_meta_train, X_meta_val, y_train, y_val = train_test_split(
    X_img_temp, X_meta_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)

# ğŸ§  CNN for Feature Extraction (no final classifier layer)
image_input = Input(shape=(128, 128, 3), name="stft_image")
x = layers.Conv2D(32, 3, activation='relu', padding='same')(image_input)
x = layers.MaxPooling2D(2)(x)
x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D(2)(x)
x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D(2)(x)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.4)(x)
cnn_model = models.Model(inputs=image_input, outputs=x)

# ğŸ“¤ Extract CNN features
cnn_train = cnn_model.predict(X_img_train)
cnn_val = cnn_model.predict(X_img_val)
cnn_test = cnn_model.predict(X_img_test)

# ğŸ”— Combine CNN features + metadata
X_train_svm = np.concatenate([cnn_train, X_meta_train], axis=1)
X_val_svm = np.concatenate([cnn_val, X_meta_val], axis=1)
X_test_svm = np.concatenate([cnn_test, X_meta_test], axis=1)

# ğŸ¯ Train SVM
svm = SVC(kernel='rbf', probability=True)
svm.fit(X_train_svm, y_train)

# ğŸ“ˆ Evaluate
y_pred = svm.predict(X_test_svm)
print(f"âœ… SVM Test Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")
print("\nğŸ“‹ Classification Report:\n", classification_report(y_test, y_pred))

# Uninstall potentially conflicting versions
!pip uninstall -y scipy numpy

# Reinstall scipy to get a compatible version
!pip install scipy

import matplotlib.pyplot as plt

# Select the first image from the dataset
stft_image = X_images[1]

# Display the image
plt.figure(figsize=(6, 6))
plt.imshow(stft_image)
plt.title("Example STFT Image")
plt.axis('off') # Hide axes
plt.show()